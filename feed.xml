<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://howardzhan2024.top//feed.xml" rel="self" type="application/atom+xml"/><link href="https://howardzhan2024.top//" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-26T00:26:27+00:00</updated><id>https://howardzhan2024.top//feed.xml</id><title type="html">blank</title><subtitle>My name is Howard, nice to meet you. </subtitle><entry><title type="html">Why cross entropy comes in hand with Softmax layer?</title><link href="https://howardzhan2024.top//blog/2024/cross-entropy/" rel="alternate" type="text/html" title="Why cross entropy comes in hand with Softmax layer?"/><published>2024-08-23T16:40:16+00:00</published><updated>2024-08-23T16:40:16+00:00</updated><id>https://howardzhan2024.top//blog/2024/cross-entropy</id><content type="html" xml:base="https://howardzhan2024.top//blog/2024/cross-entropy/"><![CDATA[<p><strong><mark><u>Why we need to use softmax function after cross entropy?</u></mark></strong></p> <p>Because thecross entropy loss takes the <em>logatithm of the probability</em>. So in order to compute an efficient logarithm, we need to have <em>a probability distribution that sums up to 1</em>.</p>]]></content><author><name></name></author><category term="neural"/><category term="networks"/><category term="Q&amp;A"/><summary type="html"><![CDATA[Why we need to use softmax function after cross entropy?]]></summary></entry><entry><title type="html">Is KL divergence same as cross entropy for image classification?</title><link href="https://howardzhan2024.top//blog/2024/cross-entropy-copy/" rel="alternate" type="text/html" title="Is KL divergence same as cross entropy for image classification?"/><published>2024-08-23T16:40:16+00:00</published><updated>2024-08-23T16:40:16+00:00</updated><id>https://howardzhan2024.top//blog/2024/cross-entropy-copy</id><content type="html" xml:base="https://howardzhan2024.top//blog/2024/cross-entropy-copy/"><![CDATA[<p><strong><mark><u>Yes</u></mark></strong></p>]]></content><author><name></name></author><category term="neural"/><category term="networks"/><category term="Q&amp;A"/><summary type="html"><![CDATA[Yes]]></summary></entry><entry><title type="html">Joined and be an event volunteer in the conference of Goodle cloud of Toronto</title><link href="https://howardzhan2024.top//blog/2024/google-cloud-of-Toronto/" rel="alternate" type="text/html" title="Joined and be an event volunteer in the conference of Goodle cloud of Toronto"/><published>2024-06-16T16:40:16+00:00</published><updated>2024-06-16T16:40:16+00:00</updated><id>https://howardzhan2024.top//blog/2024/google-cloud-of-Toronto</id><content type="html" xml:base="https://howardzhan2024.top//blog/2024/google-cloud-of-Toronto/"><![CDATA[<p>Joined and be an event volunteer in the conference of Goodle cloud of Toronto.</p>]]></content><author><name></name></author><category term="activity"/><category term="Google"/><summary type="html"><![CDATA[Joined and be an event volunteer in the conference of Goodle cloud of Toronto.]]></summary></entry></feed>